{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Demo for Barcelona\n",
    "\n",
    "> ☝️ This notebook is prepared for working in local\n",
    "\n",
    "---\n",
    "\n",
    "In this tutorial, ..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Inicializando el entorno\n",
    "! ./setup.sh"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Context Info\n",
    "\n",
    "Vamos a darle contexto al caso de uso:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "aws_profile_name = 'INCLUDE_YOUR_AWS_PROFILE_NAME'\n",
    "aws_bedrock_assume_role = \"INCLUDE_YOUR_AWS_BEDROCK_ASSUME_ROLE_ARN\"\n",
    "\n",
    "text_to_text_model_id = \"anthropic.claude-3-sonnet-20240229-v1:0\"\n",
    "text_to_image_model_id = \"amazon.titan-image-generator-v1\"\n",
    "\n",
    "reference_url = 'https://doit.com'\n",
    "market_research_data = \"\"\"\n",
    "Red hats with large feathers is a trend in the Australian market\n",
    "\"\"\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fase 0: Capturar pantalla para branding\n",
    "---"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from selenium import webdriver\n",
    "from selenium.webdriver.firefox.service import Service as FirefoxService\n",
    "from webdriver_manager.firefox import GeckoDriverManager\n",
    "from PIL import Image\n",
    "\n",
    "image_name=\"target_snapshot.png\"\n",
    "\n",
    "def create_snapshot(url):\n",
    "    # Configurar Selenium para usar Chrome\n",
    "    driver = webdriver.Firefox(service=FirefoxService(GeckoDriverManager().install()))\n",
    "\n",
    "    # Acceder a la URL\n",
    "    driver.get(url)\n",
    "\n",
    "    # Dar tiempo a la página para que se cargue completamente\n",
    "    driver.implicitly_wait(10)  # Espera implícita de 10 segundos\n",
    "\n",
    "    # Tomar el screenshot y guardarlo\n",
    "    driver.save_screenshot(image_name)\n",
    "\n",
    "    # Cerrar el navegador\n",
    "    driver.quit()\n",
    "\n",
    "create_snapshot(reference_url)\n",
    "\n",
    "screenshot = Image.open(image_name)\n",
    "\n",
    "# Display\n",
    "screenshot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fase 1: Crear CSS a partir de la captura\n",
    "---\n",
    "Vamos a tomar el screenshot y extraer a través de **Claude 3** con el **modo vision** los principales colores para utilizarlo en la CSS de la Landing."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sys\n",
    "import base64\n",
    "\n",
    "from langchain_community.chat_models import BedrockChat\n",
    "from langchain_core.messages import HumanMessage\n",
    "from pathlib import Path\n",
    "\n",
    "from utils import bedrock\n",
    "\n",
    "os.environ[\"AWS_PROFILE\"] = aws_profile_name\n",
    "os.environ[\"BEDROCK_ASSUME_ROLE\"] = aws_bedrock_assume_role  # E.g. \"arn:aws:...\"\n",
    "model_id = text_to_text_model_id\n",
    "\n",
    "pathCSS=\"target.css\"\n",
    "\n",
    "boto3_bedrock = bedrock.get_bedrock_client(\n",
    "    assumed_role=os.environ.get(\"BEDROCK_ASSUME_ROLE\", None),\n",
    "    region=os.environ.get(\"AWS_DEFAULT_REGION\", None)\n",
    ")\n",
    "\n",
    "model_kwargs =  { \n",
    "    \"max_tokens\": 2048,\n",
    "    \"temperature\": 0.0,\n",
    "    \"top_k\": 250,\n",
    "    \"top_p\": 1,\n",
    "    \"stop_sequences\": [\"\\n\\nHuman\"],\n",
    "}\n",
    "\n",
    "model = BedrockChat(\n",
    "    client=boto3_bedrock,\n",
    "    model_id=model_id,\n",
    "    model_kwargs=model_kwargs,\n",
    ")\n",
    "\n",
    "img1_path = Path(image_name)\n",
    "img1_base64 = base64.b64encode(img1_path.read_bytes()).decode(\"utf-8\")\n",
    "\n",
    "messages = [\n",
    "    HumanMessage(\n",
    "        content = [\n",
    "            {\n",
    "                \"type\": \"image_url\",\n",
    "                \"image_url\": {\n",
    "                    \"url\": f\"data:image/png;base64,{img1_base64}\",\n",
    "                },\n",
    "            },\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": \"\"\"Extract the corporate image and colors from the image, please note that the color in texts cannot be white as , and create a snippet like this one:\n",
    "                :root {\n",
    "                    --textDark: rgba(38, 50, 56, 1);\n",
    "                    --textMedium: rgba(38, 50, 56, 0.7);\n",
    "                    --borderMedium: rgba(38, 50, 56, 0.2); \n",
    "                    --borderLight: rgba(38, 50, 56, 0.075);\n",
    "                    --accent: #f4b500;\n",
    "                    --textAccent: #e6ac00;\n",
    "                    --light: rgba(38, 50, 56, 0.035);\n",
    "                }\n",
    "                \"\"\"\n",
    "            },\n",
    "        ]\n",
    "    )\n",
    "]\n",
    "\n",
    "# Abre el archivo donde deseas guardar el contenido\n",
    "with open(pathCSS, 'a') as f:\n",
    "    for chunk in model.stream(messages):\n",
    "        # Imprime el contenido en la consola\n",
    "        print(chunk.content, end=\"\", flush=True)\n",
    "        # Escribe el mismo contenido en el archivo\n",
    "        f.write(chunk.content)\n",
    "        # Asegura que el contenido se escribe inmediatamente en el archivo\n",
    "        f.flush()\n",
    "        # Opcionalmente, puedes usar sys.stdout.flush() para asegurar que la salida se muestre inmediatamente en la consola\n",
    "        sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fase 2: Actualizar CSS\n",
    "---\n",
    "Vamos a introducir la configuración de la CSS en el fichero adecuado de Hugobricks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def sustituir_variables(css_original_path, nuevas_variables_path):\n",
    "    # Leer el contenido del archivo CSS original\n",
    "    with open(css_original_path, 'r') as file:\n",
    "        contenido_original = file.read()\n",
    "\n",
    "    # Leer el archivo con las nuevas variables y sus valores\n",
    "    with open(nuevas_variables_path, 'r') as file:\n",
    "        nuevas_variables = file.read()\n",
    "\n",
    "    # Extraer las variables y sus valores del archivo de nuevas variables\n",
    "    variables_dict = {}\n",
    "    for line in nuevas_variables.splitlines():\n",
    "        match = re.search(r'(--\\w+):\\s*(.*);', line)\n",
    "        if match:\n",
    "            variable, valor = match.groups()\n",
    "            variables_dict[variable] = valor\n",
    "\n",
    "    # Sustituir los valores de las variables en el contenido original\n",
    "    for variable, nuevo_valor in variables_dict.items():\n",
    "        contenido_original = re.sub(rf'({variable}:\\s*).+?;', rf'\\1{nuevo_valor};', contenido_original)\n",
    "\n",
    "    # Guardar los cambios en el archivo CSS original\n",
    "    with open(css_original_path, 'w') as file:\n",
    "        file.write(contenido_original)\n",
    "\n",
    "    print(\"Las variables han sido sustituidas correctamente.\")\n",
    "\n",
    "# Rutas de los archivos (ajustalas según tu estructura de directorios)\n",
    "css_original_path = 'hugobricks/static/css/style.css'\n",
    "nuevas_variables_path = 'target.css'\n",
    "\n",
    "# Llamar a la función para realizar la sustitución\n",
    "sustituir_variables(css_original_path, nuevas_variables_path)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! head -18 hugobricks/static/css/style.css | tail -14"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fase 3: Generar el contenido de la propuesta de valor\n",
    "---\n",
    "\n",
    "En base al estudio de mercado vamos a generar el contenido de nuestra propuesta de valor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from langchain_core.prompts import PromptTemplate\n",
    "from langchain_core.messages import HumanMessage\n",
    "from pathlib import Path\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "                Imagine we are launching a new product based of some new market research we have been doing\n",
    "                [\n",
    "                {market_research_data}\n",
    "                ]\n",
    "                we would like to make a landing page for testing the market, I would like you to help me to\n",
    "                create the value proposal with all the info with the following hugobricks template:\n",
    "                [\n",
    "                {vanilla_template}\n",
    "                ]\n",
    "                This is an example of how it should look like the template once the data is populated.\n",
    "                Notice that the sections brick_reviews and brick_cta have been removed from the template, hence you should do the same on your response:\n",
    "                [\n",
    "                {real_example}\n",
    "                ]\n",
    "                I don't need any explanation, I need you to create the same format but changed the data based on the information from the market research, invent whatever you need. Watch out for offensive content, illegal or any other language that could damage our reputation.\n",
    "                All the call to action buttons, change the caption to whatever you want but they should point to /get-started/ as this is going to be one single page.\n",
    "                Produce only the template without any further explanation, so that I can copy and paste the file.\n",
    "                The brick_cta module you do not need to populate it.\n",
    "                \"\"\"\n",
    "\n",
    "def create_prompt(prompt_template, market_research_data, vanilla_template_path, real_example_path):\n",
    "    # Leer el contenido del primer archivo\n",
    "    with open(vanilla_template_path, 'r') as vanilla_template_file:\n",
    "        vanilla_template = vanilla_template_file.read()\n",
    "\n",
    "    # Leer el contenido del segundo archivo\n",
    "    with open(real_example_path, 'r') as real_example_file:\n",
    "        real_example = real_example_file.read()\n",
    "\n",
    "    prompt = PromptTemplate.from_template(prompt_template)\n",
    "    prompt_text = prompt.format(market_research_data=market_research_data,\n",
    "                                vanilla_template=vanilla_template,\n",
    "                                real_example=real_example\n",
    "                                )\n",
    "\n",
    "    return prompt_text\n",
    "\n",
    "# Rutas de los archivos de ejemplo\n",
    "vanilla_template_path = 'index_template.md'\n",
    "real_example_path = 'hugobricks/content/en/_index.md'\n",
    "pathIndex = \"target_index.md\"\n",
    "\n",
    "# Llamar a la función y pasar las rutas de los archivos como argumentos\n",
    "prompt = create_prompt(prompt_template, market_research_data, vanilla_template_path, real_example_path)\n",
    "\n",
    "messages = [\n",
    "    HumanMessage(\n",
    "        content = [\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": prompt\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "]\n",
    "\n",
    "# Abre el archivo donde deseas guardar el contenido\n",
    "with open(pathIndex, 'a') as f:\n",
    "    for chunk in model.stream(messages):\n",
    "        # Imprime el contenido en la consola\n",
    "        print(chunk.content, end=\"\", flush=True)\n",
    "        # Escribe el mismo contenido en el archivo\n",
    "        f.write(chunk.content)\n",
    "        # Asegura que el contenido se escribe inmediatamente en el archivo\n",
    "        f.flush()\n",
    "        # Opcionalmente, puedes usar sys.stdout.flush() para asegurar que la salida se muestre inmediatamente en la consola\n",
    "        sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! cp -v target_index.md hugobricks/content/en/_index.md"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fase 4: Crear los conceptos de las imagenes\n",
    "---\n",
    "Con Claude 3 vamos a generar los conceptos de las imágenes que luego le pediremos a Titan que genere.\n",
    "\n",
    "NOTA: Estamos utilizando los mismos parametros del modelo para generar el texto, pero podríamos ser más creativos en algunas partes que en otras cambiando la temperatura, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_concepts_json = \"target_images_concepts.json\"\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "                Act as an expert in Marketing and design. Create the image's descriptions that we will pass to a text-to-image model for creating\n",
    "                the images to attach to our value proposal. This is the value proposal\n",
    "                [\n",
    "                {value_proposal_data}\n",
    "                ]\n",
    "                Include the prompt and the negative prompt (this are the things you want to avoid in the image generation). Please do not include text on the images\n",
    "                For example:\n",
    "                [\n",
    "                prompt = \"a beautiful lake surrounded by trees with a mountain range at the distance\"\n",
    "                negative_prompts = \"poorly rendered, poor background details, poorly drawn mountains, disfigured mountain features\"\n",
    "                ]\n",
    "                I am expeciting 4 images: \n",
    "                [\n",
    "                    - One image for the logo\n",
    "                    - One image for the intro section\n",
    "                    - One image for the bimage2 section\n",
    "                    - One image for the bimage section\n",
    "                ]\n",
    "                And return all the descriptions in a json format, be sure that the object is in the first position of an array, as follows: on\n",
    "                [\n",
    "                {{\n",
    "                    \"logo\":{{\n",
    "                        \"filename\": \"logo.png\",\n",
    "                        \"prompt\": \"prompt description for the logo generation\",\n",
    "                        \"negative_prompts\": \"negative prompt description for the logo generation\",\n",
    "                    }},\n",
    "                    \"intro\":{{\n",
    "                        \"filename\": \"intro.png\",\n",
    "                        \"prompt\": \"prompt description for the intro generation\",\n",
    "                        \"negative_prompts\": \"negative prompt description for the intro generation\",\n",
    "                    }},\n",
    "                    \"bimage2\":{{\n",
    "                        \"filename\": \"bimage2.png\",\n",
    "                        \"prompt\": \"prompt description for the bimage2 generation\",\n",
    "                        \"negative_prompts\": \"negative prompt description for the bimage2 generation\",\n",
    "                    }},\n",
    "                    \"bimage\":{{\n",
    "                        \"filename\": \"baimage.png\",\n",
    "                        \"prompt\": \"prompt description for the bimage generation\",\n",
    "                        \"negative_prompts\": \"negative prompt description for the bimage generation\",\n",
    "                    }}\n",
    "                }}\n",
    "                ]\n",
    "                I don't need any explanation, I need you to create the same format but changed the data based on the information from the value proposal, invent whatever you need. Watch out for offensive content, illegal or any other language that could damage our reputation.\n",
    "                Produce only the json without any further explanation, so that I can copy and paste the file.\n",
    "                \"\"\"\n",
    "\n",
    "def create_prompt(prompt_template, value_proposal_path):\n",
    "    # Leer el contenido del primer archivo\n",
    "    with open(value_proposal_path, 'r') as value_proposal_file:\n",
    "        value_proposal = value_proposal_file.read()\n",
    "\n",
    "    prompt = PromptTemplate.from_template(prompt_template)\n",
    "    prompt_text = prompt.format(value_proposal_data=value_proposal)\n",
    "\n",
    "    return prompt_text\n",
    "\n",
    "value_proposal_path = pathIndex\n",
    "\n",
    "prompt = create_prompt(prompt_template, value_proposal_path)\n",
    "\n",
    "messages = [\n",
    "    HumanMessage(\n",
    "        content = [\n",
    "            {\n",
    "                \"type\": \"text\",\n",
    "                \"text\": prompt\n",
    "            }\n",
    "        ]\n",
    "    )\n",
    "]\n",
    "\n",
    "# Abre el archivo donde deseas guardar el contenido\n",
    "with open(image_concepts_json, 'a') as f:\n",
    "    for chunk in model.stream(messages):\n",
    "        # Imprime el contenido en la consola\n",
    "        print(chunk.content, end=\"\", flush=True)\n",
    "        # Escribe el mismo contenido en el archivo\n",
    "        f.write(chunk.content)\n",
    "        # Asegura que el contenido se escribe inmediatamente en el archivo\n",
    "        f.flush()\n",
    "        # Opcionalmente, puedes usar sys.stdout.flush() para asegurar que la salida se muestre inmediatamente en la consola\n",
    "        sys.stdout.flush()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fase 5: Crear las images basadas en los conceptos\n",
    "---\n",
    "\n",
    "Vamos a utilizar Amazon Titan como modelo de text-to-image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import base64\n",
    "import io\n",
    "import json\n",
    "\n",
    "images_dir= \"demo_images\"\n",
    "\n",
    "# Leer y cargar el contenido del archivo JSON\n",
    "with open(image_concepts_json, 'r') as file:\n",
    "    images_json = json.load(file)\n",
    "\n",
    "def create_body(prompt, negative_prompts):\n",
    "    body = json.dumps(\n",
    "    {\n",
    "        \"taskType\": \"TEXT_IMAGE\",\n",
    "        \"textToImageParams\": {\n",
    "            \"text\": prompt,                    # Required\n",
    "            \"negativeText\": negative_prompts   # Optional\n",
    "        },\n",
    "        \"imageGenerationConfig\": {\n",
    "            \"numberOfImages\": 1,   # Range: 1 to 5 \n",
    "            \"quality\": \"standard\",  # Options: standard or premium\n",
    "            \"height\": 1024,        # Supported height list in the docs \n",
    "            \"width\": 1024,         # Supported width list in the docs\n",
    "            \"cfgScale\": 7.5,       # Range: 1.0 (exclusive) to 10.0\n",
    "            \"seed\": 42             # Range: 0 to 214783647\n",
    "        }\n",
    "    }\n",
    "    )\n",
    "    return body\n",
    "\n",
    "# Función para hacer una llamada a la API\n",
    "def create_image(seccion, data):\n",
    "    \n",
    "    body = create_body(data['prompt'],data['negative_prompts'])\n",
    "\n",
    "    #print(body)\n",
    "    # Make model request\n",
    "    response = boto3_bedrock.invoke_model(\n",
    "        body=body,\n",
    "        modelId=text_to_image_model_id,\n",
    "        accept=\"application/json\", \n",
    "        contentType=\"application/json\"\n",
    "    )\n",
    "\n",
    "    # Process the image\n",
    "    response_body = json.loads(response.get(\"body\").read())\n",
    "    img1_b64 = response_body[\"images\"][0]\n",
    "\n",
    "    os.makedirs(images_dir, exist_ok=True)\n",
    "\n",
    "    # Decode + save\n",
    "    img1 = Image.open(\n",
    "        io.BytesIO(\n",
    "            base64.decodebytes(\n",
    "                bytes(img1_b64, \"utf-8\")\n",
    "            )\n",
    "        )\n",
    "    )\n",
    "    img1.save(f\"{images_dir}/{data['filename']}\")\n",
    "    img1\n",
    "    \n",
    "\n",
    "# Iterar sobre cada objeto en el JSON\n",
    "for object in images_json:\n",
    "    # Iterar sobre cada sección dentro del objeto\n",
    "    for image in object:\n",
    "        create_image(image, object[image])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%sh\n",
    "mkdir -p hugobricks/static/uploads/illustrations/demo/\n",
    "cp demo_images/* hugobricks/static/uploads/illustrations/demo/\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import subprocess\n",
    "\n",
    "executable_directory = \"hugobricks\"\n",
    "executable_name = \"hugo\"\n",
    "arguments = [\"server\"]\n",
    "process = subprocess.Popen([executable_name] + arguments, stdout=subprocess.PIPE, stderr=subprocess.PIPE, cwd=executable_directory)\n",
    "process_id = process.pid\n",
    "print(f\"Background process started with PID: {process_id} from directory {executable_directory}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re, time\n",
    "\n",
    "time.sleep(2)\n",
    "command = [\"lsof\", \"-nP\", \"-i4TCP\", \"-a\", f\"-p{process_id}\"]\n",
    "lsof_output = subprocess.Popen(command, stdout=subprocess.PIPE, stderr=subprocess.PIPE)\n",
    "stdout, stderr = lsof_output.communicate()\n",
    "output = stdout.decode('utf-8')\n",
    "\n",
    "if output:\n",
    "    print(\"Open ports by the process:\")\n",
    "    print(output)\n",
    "    pattern = r'\\b(?:[0-9]{1,3}\\.){3}[0-9]{1,3}:[0-9]+\\b'\n",
    "    match = re.search(pattern, output)\n",
    "    if match:\n",
    "        ip_port = match.group()\n",
    "        print(\"http://\" + ip_port)\n",
    "    else:\n",
    "        print(\"No IP address and port found in the output.\")\n",
    "else:\n",
    "    print(\"No open ports found for the process or unable to retrieve information.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "<br>\n",
    "\n",
    "Clean up\n",
    "---\n",
    "\n",
    "Parando Hugo Server"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import signal\n",
    "\n",
    "from IPython.display import display\n",
    "import ipywidgets as widgets\n",
    "\n",
    "def confirm_action(b):\n",
    "    print(\"User confirmed. Proceeding with the destructive action...\")\n",
    "    os.kill(process_id, signal.SIGTERM)\n",
    "    print(f\"Background process with PID {process_id} has been terminated.\")\n",
    "    confirm_button.disabled = True\n",
    "    deny_button.disabled = True\n",
    "\n",
    "def deny_action(b):\n",
    "    print(\"User did not confirm. Hugo Server is still running.\")\n",
    "    confirm_button.disabled = True\n",
    "    deny_button.disabled = True\n",
    "\n",
    "\n",
    "message_label = widgets.Label(\"Do you want to stop the Hugo Server?\")\n",
    "\n",
    "confirm_button = widgets.Button(description=\"Confirm\")\n",
    "deny_button = widgets.Button(description=\"Deny\")\n",
    "\n",
    "confirm_button.on_click(confirm_action)\n",
    "deny_button.on_click(deny_action)\n",
    "\n",
    "display(widgets.VBox([message_label, widgets.HBox([confirm_button, deny_button])]))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "availableInstances": [
   {
    "_defaultOrder": 0,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.t3.medium",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 1,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.t3.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 2,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.t3.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 3,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.t3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 4,
    "_isFastLaunch": true,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 5,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 6,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 7,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 8,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 9,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 10,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 11,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 12,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.m5d.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 13,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.m5d.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 14,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.m5d.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 15,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.m5d.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 16,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.m5d.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 17,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.m5d.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 18,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.m5d.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 19,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.m5d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 20,
    "_isFastLaunch": false,
    "category": "General purpose",
    "gpuNum": 0,
    "hideHardwareSpecs": true,
    "memoryGiB": 0,
    "name": "ml.geospatial.interactive",
    "supportedImageNames": [
     "sagemaker-geospatial-v1-0"
    ],
    "vcpuNum": 0
   },
   {
    "_defaultOrder": 21,
    "_isFastLaunch": true,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 4,
    "name": "ml.c5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 22,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 8,
    "name": "ml.c5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 23,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.c5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 24,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.c5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 25,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 72,
    "name": "ml.c5.9xlarge",
    "vcpuNum": 36
   },
   {
    "_defaultOrder": 26,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 96,
    "name": "ml.c5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 27,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 144,
    "name": "ml.c5.18xlarge",
    "vcpuNum": 72
   },
   {
    "_defaultOrder": 28,
    "_isFastLaunch": false,
    "category": "Compute optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.c5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 29,
    "_isFastLaunch": true,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g4dn.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 30,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g4dn.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 31,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g4dn.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 32,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g4dn.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 33,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g4dn.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 34,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g4dn.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 35,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 61,
    "name": "ml.p3.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 36,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 244,
    "name": "ml.p3.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 37,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 488,
    "name": "ml.p3.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 38,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.p3dn.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 39,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.r5.large",
    "vcpuNum": 2
   },
   {
    "_defaultOrder": 40,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.r5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 41,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.r5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 42,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.r5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 43,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.r5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 44,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.r5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 45,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.r5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 46,
    "_isFastLaunch": false,
    "category": "Memory Optimized",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.r5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 47,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 16,
    "name": "ml.g5.xlarge",
    "vcpuNum": 4
   },
   {
    "_defaultOrder": 48,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.g5.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 49,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 64,
    "name": "ml.g5.4xlarge",
    "vcpuNum": 16
   },
   {
    "_defaultOrder": 50,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 128,
    "name": "ml.g5.8xlarge",
    "vcpuNum": 32
   },
   {
    "_defaultOrder": 51,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 1,
    "hideHardwareSpecs": false,
    "memoryGiB": 256,
    "name": "ml.g5.16xlarge",
    "vcpuNum": 64
   },
   {
    "_defaultOrder": 52,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 192,
    "name": "ml.g5.12xlarge",
    "vcpuNum": 48
   },
   {
    "_defaultOrder": 53,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 4,
    "hideHardwareSpecs": false,
    "memoryGiB": 384,
    "name": "ml.g5.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 54,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 768,
    "name": "ml.g5.48xlarge",
    "vcpuNum": 192
   },
   {
    "_defaultOrder": 55,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4d.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 56,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 8,
    "hideHardwareSpecs": false,
    "memoryGiB": 1152,
    "name": "ml.p4de.24xlarge",
    "vcpuNum": 96
   },
   {
    "_defaultOrder": 57,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 32,
    "name": "ml.trn1.2xlarge",
    "vcpuNum": 8
   },
   {
    "_defaultOrder": 58,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1.32xlarge",
    "vcpuNum": 128
   },
   {
    "_defaultOrder": 59,
    "_isFastLaunch": false,
    "category": "Accelerated computing",
    "gpuNum": 0,
    "hideHardwareSpecs": false,
    "memoryGiB": 512,
    "name": "ml.trn1n.32xlarge",
    "vcpuNum": 128
   }
  ],
  "instance_type": "ml.t3.medium",
  "kernelspec": {
   "display_name": ".env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "nbdime-conflicts": {
   "local_diff": [
    {
     "diff": [
      {
       "diff": [
        {
         "key": 0,
         "length": 1,
         "op": "removerange"
        }
       ],
       "key": "name",
       "op": "patch"
      }
     ],
     "key": "kernelspec",
     "op": "patch"
    }
   ],
   "remote_diff": [
    {
     "diff": [
      {
       "diff": [
        {
         "diff": [
          {
           "key": 50,
           "length": 1,
           "op": "removerange"
          },
          {
           "key": 52,
           "op": "addrange",
           "valuelist": "a"
          },
          {
           "key": 55,
           "op": "addrange",
           "valuelist": "1:0813"
          },
          {
           "key": 56,
           "op": "addrange",
           "valuelist": "5"
          },
          {
           "key": 56,
           "length": 2,
           "op": "removerange"
          },
          {
           "key": 59,
           "op": "addrange",
           "valuelist": "90"
          },
          {
           "key": 59,
           "length": 2,
           "op": "removerange"
          },
          {
           "key": 62,
           "op": "addrange",
           "valuelist": "99"
          },
          {
           "key": 62,
           "length": 7,
           "op": "removerange"
          }
         ],
         "key": 0,
         "op": "patch"
        }
       ],
       "key": "name",
       "op": "patch"
      }
     ],
     "key": "kernelspec",
     "op": "patch"
    }
   ]
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
